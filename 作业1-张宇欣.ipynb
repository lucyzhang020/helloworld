{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用梯度下降法计算线性回归表达式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算目标函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于误差向量 $\\epsilon=Xb-y$，我们希望最小化它的长度 $\\epsilon^{T}\\epsilon$,代入得到目标函数为"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$min_{b} (Xb-y)^{T}(Xb-y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中 $X$ 是解释变量矩阵，第一列为全是1的向量，其余各列为不同特征具体取值的向量。$b$ 是需要计算的回归参数，$y$ 是记录的标签。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  计算目标函数的梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对目标函数 $f(b)=(Xb-y)^{T}(Xb-y)$ 中的 $b$ 计算梯度得"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla_{b} f =2X^{T}Xb-2X^{T}y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过代入 $b$ 的值得到目标函数在该点梯度向量，用于迭代进行梯度下降计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算每一次迭代后的梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "记 $b_{n}, b_{n+1}$ 为迭代前后的线性回归系数； $d=b_{n+1}-b_{n}$ 为前进的方向。使用泰勒一阶展开式 $f(b_{n+1})=f(b_{n})+\\nabla_{b_n} f^{T}(b_{n+1}-b_{n})$, 当 $d$ 的方向和梯度向量相反的时候 $f(b_{n+1})-f(b_{n})$ 取值为负，目标函数下降的幅度最大。当取步长为 $\\alpha$ 时，迭代关系式为"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$b_{n+1}=b_{n}-\\alpha\\nabla_{b_n} f$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "于是迭代后的梯度向量为 $\\nabla_{b_{n+1}}\\  f =2X^{T}Xb_{n+1}-2X^{T}y$， 如果迭代后的梯度向量长度小于0.01就认为达到局部极小点并返回当前 $b$ 的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.4975986 ],\n",
       "        [ 1.39651535],\n",
       "        [-0.61309023]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def linearReg(x1,x2,y):\n",
    "    import numpy as np\n",
    "    X=np.column_stack([np.ones((len(x1),1)),x1,x2])   #按列串联生成解释变量矩阵\n",
    "    y=np.matrix(y).T              #把标签转为列向量\n",
    "    b=np.matrix(np.ones((3,1)))     #把线性回归系数全部初始化为1\n",
    "    X_trans_X=np.dot(X.T,X)                #计算X的转秩和X的矩阵乘积，用dot()函数\n",
    "    gradient=2*X_trans_X*b-2*X.T*y         #计算梯度向量\n",
    "    \n",
    "    while gradient.T*gradient>0.01:        \n",
    "        b=b-0.0001*gradient               #步长为0.0001\n",
    "        gradient=2*X_trans_X*b-2*X.T*y\n",
    "    return b\n",
    "\n",
    "linearReg([1,2,3,4],[5,6,7,8],[-1,-1,1,1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
